{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pydicom\nimport os\nimport collections\nimport sys\nimport glob\nimport random\nimport cv2\nimport tensorflow as tf\nimport multiprocessing\n\nfrom math import ceil, floor\nfrom copy import deepcopy\nfrom tqdm import tqdm_notebook as tqdm\nfrom imgaug import augmenters as iaa\n\nimport keras\nimport keras.backend as K\nfrom keras.callbacks import Callback, ModelCheckpoint\nfrom keras.layers import Dense, Flatten, Dropout\nfrom keras.models import Model, load_model\nfrom keras.utils import Sequence\nfrom keras.losses import binary_crossentropy\nfrom keras.optimizers import Adam","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install efficientnet\n!pip install iterative-stratification","execution_count":10,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: efficientnet in /opt/conda/lib/python3.6/site-packages (1.0.0)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.6/site-packages (from efficientnet) (0.16.2)\nRequirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /opt/conda/lib/python3.6/site-packages (from efficientnet) (1.0.8)\nRequirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet) (2.4)\nRequirement already satisfied: scipy>=0.19.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet) (1.4.1)\nRequirement already satisfied: pillow>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet) (5.4.1)\nRequirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet) (3.0.3)\nRequirement already satisfied: PyWavelets>=0.4.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet) (1.1.1)\nRequirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet) (2.6.1)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (2.9.0)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.6/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.18.1)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from networkx>=2.0->scikit-image->efficientnet) (4.4.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (0.10.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (1.1.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.4.5)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.8.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.13.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (42.0.2.post20191203)\nRequirement already satisfied: iterative-stratification in /opt/conda/lib/python3.6/site-packages (0.1.6)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from iterative-stratification) (1.4.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from iterative-stratification) (1.18.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from iterative-stratification) (0.21.3)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->iterative-stratification) (0.14.1)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Custom Modules\nimport efficientnet.keras as efn \nfrom iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seed\nSEED = 12345\nnp.random.seed(SEED)\n#tf.set_random_seed(SEED)\ntf.random.set_seed(SEED)\n\n# Constants\nTEST_SIZE = 0.02\nHEIGHT = 256\nWIDTH = 256\nCHANNELS = 3\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 64\nSHAPE = (HEIGHT, WIDTH, CHANNELS)\n\n# Folders\nDATA_DIR = '/kaggle/input/new-csvs/'\nDATA_DIR2 = '/kaggle/input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/'\nTEST_IMAGES_DIR = DATA_DIR2 + 'stage_2_test/'\nTRAIN_IMAGES_DIR = DATA_DIR2 + 'stage_2_train/'","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def correct_dcm(dcm):\n    x = dcm.pixel_array + 1000\n    px_mode = 4096\n    x[x>=px_mode] = x[x>=px_mode] - px_mode\n    dcm.PixelData = x.tobytes()\n    dcm.RescaleIntercept = -1000\n\ndef window_image(dcm, window_center, window_width):    \n    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n        correct_dcm(dcm)\n    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n    \n    # Resize\n    img = cv2.resize(img, SHAPE[:2], interpolation = cv2.INTER_LINEAR)\n   \n    img_min = window_center - window_width // 2\n    img_max = window_center + window_width // 2\n    img = np.clip(img, img_min, img_max)\n    return img\n\ndef bsb_window(dcm):\n    brain_img = window_image(dcm, 40, 80)\n    subdural_img = window_image(dcm, 80, 200)\n    soft_img = window_image(dcm, 40, 380)\n    \n    brain_img = (brain_img - 0) / 80\n    subdural_img = (subdural_img - (-20)) / 200\n    soft_img = (soft_img - (-150)) / 380\n    bsb_img = np.array([brain_img, subdural_img, soft_img]).transpose(1,2,0)\n    return bsb_img\n\ndef _read(path, SHAPE):\n    dcm = pydicom.dcmread(path)\n    try:\n        img = bsb_window(dcm)\n    except:\n        img = np.zeros(SHAPE)\n    return img","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Image Augmentation\nsometimes = lambda aug: iaa.Sometimes(0.25, aug)\naugmentation = iaa.Sequential([ iaa.Fliplr(0.25),\n                                iaa.Flipud(0.10),\n                                sometimes(iaa.Crop(px=(0, 25), keep_size = True, sample_independently = False))   \n                            ], random_order = True)       \n        \n# Generators\nclass TrainDataGenerator(keras.utils.Sequence):\n    def __init__(self, dataset, labels, batch_size = 16, img_size = SHAPE, img_dir = TRAIN_IMAGES_DIR, augment = False, *args, **kwargs):\n        self.dataset = dataset\n        self.ids = dataset.index\n        self.labels = labels\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.img_dir = img_dir\n        self.augment = augment\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(ceil(len(self.ids) / self.batch_size))\n\n    def __getitem__(self, index):\n        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n        X, Y = self.__data_generation(indices)\n        return X, Y\n\n    def augmentor(self, image):\n        augment_img = augmentation        \n        image_aug = augment_img.augment_image(image)\n        return image_aug\n\n    def on_epoch_end(self):\n        self.indices = np.arange(len(self.ids))\n        np.random.shuffle(self.indices)\n\n    def __data_generation(self, indices):\n        X = np.empty((self.batch_size, *self.img_size))\n        Y = np.empty((self.batch_size, 6), dtype=np.float32)\n        \n        for i, index in enumerate(indices):\n            ID = self.ids[index]\n            image = _read(self.img_dir+ID+\".dcm\", self.img_size)\n            if self.augment:\n                X[i,] = self.augmentor(image)\n            else:\n                X[i,] = image\n            Y[i,] = self.labels.iloc[index].values        \n        return X, Y\n    \nclass TestDataGenerator(keras.utils.Sequence):\n    def __init__(self, dataset, labels, batch_size = 16, img_size = SHAPE, img_dir = TEST_IMAGES_DIR, *args, **kwargs):\n        self.dataset = dataset\n        self.ids = dataset.index\n        self.labels = labels\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.img_dir = img_dir\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(ceil(len(self.ids) / self.batch_size))\n\n    def __getitem__(self, index):\n        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n        X = self.__data_generation(indices)\n        return X\n\n    def on_epoch_end(self):\n        self.indices = np.arange(len(self.ids))\n    \n    def __data_generation(self, indices):\n        X = np.empty((self.batch_size, *self.img_size))\n        \n        for i, index in enumerate(indices):\n            ID = self.ids[index]\n            image = _read(self.img_dir+ID+\".dcm\", self.img_size)\n            X[i,] = image              \n        return X","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_testset(filename = DATA_DIR + \"stage_2_sample_submission.csv\"):\n    df = pd.read_csv(filename)\n    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n    return df\n\ndef read_trainset(filename = DATA_DIR + \"stage_2_train.csv\"):\n    df = pd.read_csv(filename)\n    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n    '''\n    duplicates_to_remove = [56346, 56347, 56348, 56349,\n                            56350, 56351, 1171830, 1171831,\n                            1171832, 1171833, 1171834, 1171835,\n                            3705312, 3705313, 3705314, 3705315,\n                            3705316, 3705317, 3842478, 3842479,\n                            3842480, 3842481, 3842482, 3842483 ]\n    df = df.drop(index = duplicates_to_remove)\n    '''\n    df = df.reset_index(drop = True)    \n    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n    return df\n\n# Read Train and Test Datasets\ntest_df = read_testset()\ntrain_df = read_trainset()","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Oversampling\nepidural_df = train_df[train_df.Label['epidural'] == 1]\ntrain_oversample_df = pd.concat([train_df, epidural_df])\ntrain_df = train_oversample_df\n\n# Summary\nprint('Train Shape: {}'.format(train_df.shape))\nprint('Test Shape: {}'.format(test_df.shape))","execution_count":16,"outputs":[{"output_type":"stream","text":"Train Shape: (33478, 6)\nTest Shape: (33334, 6)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predictions(test_df, model):    \n    test_preds = model.predict_generator(TestDataGenerator(test_df, None, 8, SHAPE, TEST_IMAGES_DIR), verbose = 1)\n    return test_preds[:test_df.iloc[range(test_df.shape[0])].shape[0]]\n\ndef ModelCheckpointFull(model_name):\n    return ModelCheckpoint(model_name, \n                            monitor = 'val_loss', \n                            verbose = 1, \n                            save_best_only = False, \n                            save_weights_only = True, \n                            mode = 'min', \n                            period = 1)\n\n# Create Model\ndef create_model():\n    K.clear_session()\n    \n    base_model =  efn.EfficientNetB2(weights = 'imagenet', include_top = False, pooling = 'avg', input_shape = SHAPE)\n    x = base_model.output\n    x = Dropout(0.15)(x)\n    y_pred = Dense(6, activation = 'sigmoid')(x)\n\n    return Model(inputs = base_model.input, outputs = y_pred)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" train_df.dropna(inplace=True)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submission Placeholder\nsubmission_predictions = []\n\n# Multi Label Stratified Split stuff...\nmsss = MultilabelStratifiedShuffleSplit(n_splits = 10, test_size = TEST_SIZE, random_state = SEED)\nX = train_df.index\nY = train_df.Label.values\n\n# Get train and test index\nmsss_splits = next(msss.split(X, Y))\ntrain_idx = msss_splits[0]\nvalid_idx = msss_splits[1]","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loop through Folds of Multi Label Stratified Split\nfor epoch, msss_splits in zip(range(0, 9), msss.split(X, Y)): \n#    # Get train and test index\n    train_idx = msss_splits[0]\n    valid_idx = msss_splits[1]\nfor epoch in range(0, 3):\n    print('=========== EPOCH {}'.format(epoch))\n\n    # Shuffle Train data\n    np.random.shuffle(train_idx)\n    print(train_idx[:5])    \n    print(valid_idx[:5])\n\n    # Create Data Generators for Train and Valid\n    data_generator_train = TrainDataGenerator(train_df.iloc[train_idx], \n                                                train_df.iloc[train_idx], \n                                                TRAIN_BATCH_SIZE, \n                                                SHAPE,\n                                                augment = True)\n    data_generator_val = TrainDataGenerator(train_df.iloc[valid_idx], \n                                            train_df.iloc[valid_idx], \n                                            VALID_BATCH_SIZE, \n                                            SHAPE,\n                                            augment = False)\n\n    # Create Model\n    model = create_model()\n    \n    # Full Training Model\n    for base_layer in model.layers[:-1]:\n        base_layer.trainable = True\n    TRAIN_STEPS = int(len(data_generator_train) / 6)\n    LR = 0.000125\n\n    if epoch != 0:\n        # Load Model Weights\n        model.load_weights('model.h5') \n    \n    #runoptions aqui \n    #run_opts = tf.compat.v1.RunOptions(report_tensor_allocations_upon_oom = True)\n\n    model.compile(optimizer = Adam(learning_rate = LR), \n                  loss = 'binary_crossentropy',\n                  metrics = ['acc', tf.keras.metrics.AUC()])\n    \n    # Train Model\n    model.fit_generator(generator = data_generator_train,\n                        validation_data = data_generator_val,\n                        steps_per_epoch = TRAIN_STEPS,\n                        epochs = 1,\n                        callbacks = [ModelCheckpointFull('model.h5')],\n                        verbose = 1)\n    \n    # Starting with the 4th epoch we create predictions for the test set on each epoch\n    if epoch >= 1:\n        preds = predictions(test_df, model)\n        submission_predictions.append(preds)","execution_count":20,"outputs":[{"output_type":"stream","text":"=========== EPOCH 0\n[ 3321 20935  7818 15038 31956]\n[  0  30 218 449 549]\nDownloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b2_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n31940608/31936256 [==============================] - 0s 0us/step\nEpoch 1/1\n171/171 [==============================] - 170s 994ms/step - loss: 0.2100 - acc: 0.9327 - auc: 0.6924 - val_loss: -615530019502817825259520.0000 - val_acc: 0.9411 - val_auc: 0.7897\n\nEpoch 00001: saving model to model.h5\n=========== EPOCH 1\n[27400  6953 19965 10796 27548]\n[  0  30 218 449 549]\nEpoch 1/1\n171/171 [==============================] - 163s 953ms/step - loss: 0.1365 - acc: 0.9534 - auc: 0.8913 - val_loss: 0.0426 - val_acc: 0.9600 - val_auc: 0.9107\n\nEpoch 00001: saving model to model.h5\n4167/4167 [==============================] - 381s 92ms/step\n=========== EPOCH 2\n[16378 33227 14835 24344  2949]\n[  0  30 218 449 549]\nEpoch 1/1\n171/171 [==============================] - 161s 943ms/step - loss: 0.1222 - acc: 0.9563 - auc: 0.9346 - val_loss: -52865773815747707619625937108205568.0000 - val_acc: 0.9169 - val_auc: 0.9236\n\nEpoch 00001: saving model to model.h5\n4167/4167 [==============================] - 291s 70ms/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.iloc[:, :] = np.average(submission_predictions, axis = 0, weights = [2**i for i in range(len(submission_predictions))])\ntest_df = test_df.stack().reset_index()\ntest_df.insert(loc = 0, column = 'ID', value = test_df['Image'].astype(str) + \"_\" + test_df['Diagnosis'])\ntest_df = test_df.drop([\"Image\", \"Diagnosis\"], axis=1)\ntest_df.to_csv('submission.csv', index = False)\nprint(test_df.head(12))","execution_count":21,"outputs":[{"output_type":"stream","text":"                               ID     Label\n0                ID_000176f2a_any  0.103279\n1           ID_000176f2a_epidural  0.007989\n2   ID_000176f2a_intraparenchymal  0.040635\n3   ID_000176f2a_intraventricular  0.024118\n4       ID_000176f2a_subarachnoid  0.026990\n5           ID_000176f2a_subdural  0.023795\n6                ID_00033386d_any  0.069660\n7           ID_00033386d_epidural  0.011873\n8   ID_00033386d_intraparenchymal  0.017666\n9   ID_00033386d_intraventricular  0.010098\n10      ID_00033386d_subarachnoid  0.020124\n11          ID_00033386d_subdural  0.036314\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"file_extension":".py","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"nbformat":4,"nbformat_minor":1}