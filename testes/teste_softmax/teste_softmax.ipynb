{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pydicom\nimport os\nimport collections\nimport sys\nimport glob\nimport random\nimport cv2\nimport tensorflow as tf\nimport multiprocessing\n\nfrom math import ceil, floor\nfrom copy import deepcopy\nfrom tqdm import tqdm_notebook as tqdm\nfrom imgaug import augmenters as iaa\n\nimport keras\nimport keras.backend as K\nfrom keras.callbacks import Callback, ModelCheckpoint\nfrom keras.layers import Dense, Flatten, Dropout\nfrom keras.models import Model, load_model\nfrom keras.utils import Sequence\nfrom keras.losses import binary_crossentropy\nfrom keras.optimizers import Adam","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install efficientnet\n!pip install iterative-stratification","execution_count":2,"outputs":[{"output_type":"stream","text":"^C\nTraceback (most recent call last):\n  File \"/opt/conda/bin/pip\", line 11, in <module>\n    sys.exit(main())\n  File \"/opt/conda/lib/python3.6/site-packages/pip/_internal/cli/main.py\", line 73, in main\n    command = create_command(cmd_name, isolated=(\"--isolated\" in cmd_args))\n  File \"/opt/conda/lib/python3.6/site-packages/pip/_internal/commands/__init__.py\", line 96, in create_command\n    module = importlib.import_module(module_path)\n  File \"/opt/conda/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"/opt/conda/lib/python3.6/site-packages/pip/_internal/commands/install.py\", line 24, in <module>\n    from pip._internal.cli.req_command import RequirementCommand\n  File \"/opt/conda/lib/python3.6/site-packages/pip/_internal/cli/req_command.py\", line 15, in <module>\n    from pip._internal.index.package_finder import PackageFinder\n  File \"/opt/conda/lib/python3.6/site-packages/pip/_internal/index/package_finder.py\", line 21, in <module>\n    from pip._internal.index.collector import parse_links\n  File \"/opt/conda/lib/python3.6/site-packages/pip/_internal/index/collector.py\", line 12, in <module>\n    from pip._vendor import html5lib, requests\n  File \"/opt/conda/lib/python3.6/site-packages/pip/_vendor/requests/__init__.py\", line 97, in <module>\n    from pip._vendor.urllib3.contrib import pyopenssl\n  File \"/opt/conda/lib/python3.6/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 46, in <module>\n    import OpenSSL.SSL\n  File \"/opt/conda/lib/python3.6/site-packages/OpenSSL/__init__.py\", line 8, in <module>\n    from OpenSSL import crypto, SSL\n  File \"/opt/conda/lib/python3.6/site-packages/OpenSSL/crypto.py\", line 2348, in <module>\n    class PKCS7(object):\nKeyboardInterrupt\nCollecting iterative-stratification\n  Downloading iterative_stratification-0.1.6-py3-none-any.whl (8.7 kB)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from iterative-stratification) (0.22.2.post1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from iterative-stratification) (1.18.1)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from iterative-stratification) (1.4.1)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->iterative-stratification) (0.14.1)\nInstalling collected packages: iterative-stratification\nSuccessfully installed iterative-stratification-0.1.6\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Custom Modules\nimport efficientnet.keras as efn \nfrom iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit","execution_count":3,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'efficientnet'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-dcb7554ff83d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import Custom Modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mefficientnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mefn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0miterstrat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml_stratifiers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultilabelStratifiedShuffleSplit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'efficientnet'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seed\nSEED = 12345\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\n# Constants\nTEST_SIZE = 0.02\nHEIGHT = 256\nWIDTH = 256\nCHANNELS = 3\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 64\nSHAPE = (HEIGHT, WIDTH, CHANNELS)\n\n# Folders\nDATA_DIR = '/kaggle/input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/'\nTEST_IMAGES_DIR = DATA_DIR + 'stage_2_test/'\nTRAIN_IMAGES_DIR = DATA_DIR + 'stage_2_train/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def correct_dcm(dcm):\n    x = dcm.pixel_array + 1000\n    px_mode = 4096\n    x[x>=px_mode] = x[x>=px_mode] - px_mode\n    dcm.PixelData = x.tobytes()\n    dcm.RescaleIntercept = -1000\n\ndef window_image(dcm, window_center, window_width):    \n    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n        correct_dcm(dcm)\n    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n    \n    # Resize\n    img = cv2.resize(img, SHAPE[:2], interpolation = cv2.INTER_LINEAR)\n   \n    img_min = window_center - window_width // 2\n    img_max = window_center + window_width // 2\n    img = np.clip(img, img_min, img_max)\n    return img\n\ndef bsb_window(dcm):\n    brain_img = window_image(dcm, 40, 80)\n    subdural_img = window_image(dcm, 80, 200)\n    soft_img = window_image(dcm, 40, 380)\n    \n    brain_img = (brain_img - 0) / 80\n    subdural_img = (subdural_img - (-20)) / 200\n    soft_img = (soft_img - (-150)) / 380\n    bsb_img = np.array([brain_img, subdural_img, soft_img]).transpose(1,2,0)\n    return bsb_img\n\ndef _read(path, SHAPE):\n    dcm = pydicom.dcmread(path)\n    try:\n        img = bsb_window(dcm)\n    except:\n        img = np.zeros(SHAPE)\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Image Augmentation\nsometimes = lambda aug: iaa.Sometimes(0.25, aug)\naugmentation = iaa.Sequential([ iaa.Fliplr(0.25),\n                                iaa.Flipud(0.10),\n                                sometimes(iaa.Crop(px=(0, 25), keep_size = True, sample_independently = False))   \n                            ], random_order = True)       \n        \n# Generators\nclass TrainDataGenerator(keras.utils.Sequence):\n    def __init__(self, dataset, labels, batch_size = 16, img_size = SHAPE, img_dir = TRAIN_IMAGES_DIR, augment = False, *args, **kwargs):\n        self.dataset = dataset\n        self.ids = dataset.index\n        self.labels = labels\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.img_dir = img_dir\n        self.augment = augment\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(ceil(len(self.ids) / self.batch_size))\n\n    def __getitem__(self, index):\n        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n        X, Y = self.__data_generation(indices)\n        return X, Y\n\n    def augmentor(self, image):\n        augment_img = augmentation        \n        image_aug = augment_img.augment_image(image)\n        return image_aug\n\n    def on_epoch_end(self):\n        self.indices = np.arange(len(self.ids))\n        np.random.shuffle(self.indices)\n\n    def __data_generation(self, indices):\n        X = np.empty((self.batch_size, *self.img_size))\n        Y = np.empty((self.batch_size, 6), dtype=np.float32)\n        \n        for i, index in enumerate(indices):\n            ID = self.ids[index]\n            image = _read(self.img_dir+ID+\".dcm\", self.img_size)\n            if self.augment:\n                X[i,] = self.augmentor(image)\n            else:\n                X[i,] = image\n            Y[i,] = self.labels.iloc[index].values        \n        return X, Y\n    \nclass TestDataGenerator(keras.utils.Sequence):\n    def __init__(self, dataset, labels, batch_size = 16, img_size = SHAPE, img_dir = TEST_IMAGES_DIR, *args, **kwargs):\n        self.dataset = dataset\n        self.ids = dataset.index\n        self.labels = labels\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.img_dir = img_dir\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(ceil(len(self.ids) / self.batch_size))\n\n    def __getitem__(self, index):\n        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n        X = self.__data_generation(indices)\n        return X\n\n    def on_epoch_end(self):\n        self.indices = np.arange(len(self.ids))\n    \n    def __data_generation(self, indices):\n        X = np.empty((self.batch_size, *self.img_size))\n        \n        for i, index in enumerate(indices):\n            ID = self.ids[index]\n            image = _read(self.img_dir+ID+\".dcm\", self.img_size)\n            X[i,] = image              \n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_testset(filename = DATA_DIR + \"stage_2_sample_submission.csv\"):\n    df = pd.read_csv(filename)\n    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n    return df\n\ndef read_trainset(filename = DATA_DIR + \"stage_2_train.csv\"):\n    df = pd.read_csv(filename)\n    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n    duplicates_to_remove = [56346, 56347, 56348, 56349,\n                            56350, 56351, 1171830, 1171831,\n                            1171832, 1171833, 1171834, 1171835,\n                            3705312, 3705313, 3705314, 3705315,\n                            3705316, 3705317, 3842478, 3842479,\n                            3842480, 3842481, 3842482, 3842483 ]\n    df = df.drop(index = duplicates_to_remove)\n    df = df.reset_index(drop = True)    \n    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n    return df\n\n# Read Train and Test Datasets\ntest_df = read_testset()\ntrain_df = read_trainset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Oversampling\nepidural_df = train_df[train_df.Label['epidural'] == 1]\ntrain_oversample_df = pd.concat([train_df, epidural_df])\ntrain_df = train_oversample_df\n\n# Summary\nprint('Train Shape: {}'.format(train_df.shape))\nprint('Test Shape: {}'.format(test_df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predictions(test_df, model):    \n    test_preds = model.predict_generator(TestDataGenerator(test_df, None, 8, SHAPE, TEST_IMAGES_DIR), verbose = 1)\n    return test_preds[:test_df.iloc[range(test_df.shape[0])].shape[0]]\n\ndef ModelCheckpointFull(model_name):\n    return ModelCheckpoint(model_name, \n                            monitor = 'val_loss', \n                            verbose = 1, \n                            save_best_only = False, \n                            save_weights_only = True, \n                            mode = 'min', \n                            period = 1)\n\n# Create Model\ndef create_model():\n    K.clear_session()\n    \n    base_model =  efn.EfficientNetB2(weights = 'imagenet', include_top = False, pooling = 'avg', input_shape = SHAPE)\n    x = base_model.output\n    x = Dropout(0.5)(x)\n    y_pred = Dense(6, activation = 'softmax')(x)\n\n    return Model(inputs = base_model.input, outputs = y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submission Placeholder\nsubmission_predictions = []\n\n# Multi Label Stratified Split stuff...\nmsss = MultilabelStratifiedShuffleSplit(n_splits = 10, test_size = TEST_SIZE, random_state = SEED)\nX = train_df.index\nY = train_df.Label.values\n\n# Get train and test index\nmsss_splits = next(msss.split(X, Y))\ntrain_idx = msss_splits[0]\nvalid_idx = msss_splits[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loop through Folds of Multi Label Stratified Split\n#for epoch, msss_splits in zip(range(0, 9), msss.split(X, Y)): \n#    # Get train and test index\n#    train_idx = msss_splits[0]\n#    valid_idx = msss_splits[1]\nfor epoch in range(0, 3):\n    print('=========== EPOCH {}'.format(epoch))\n\n    # Shuffle Train data\n    np.random.shuffle(train_idx)\n    print(train_idx[:5])    \n    print(valid_idx[:5])\n\n    # Create Data Generators for Train and Valid\n    data_generator_train = TrainDataGenerator(train_df.iloc[train_idx], \n                                                train_df.iloc[train_idx], \n                                                TRAIN_BATCH_SIZE, \n                                                SHAPE,\n                                                augment = True)\n    data_generator_val = TrainDataGenerator(train_df.iloc[valid_idx], \n                                            train_df.iloc[valid_idx], \n                                            VALID_BATCH_SIZE, \n                                            SHAPE,\n                                            augment = False)\n\n    # Create Model\n    model = create_model()\n    \n    # Full Training Model\n    for base_layer in model.layers[:-1]:\n        base_layer.trainable = True\n    TRAIN_STEPS = int(len(data_generator_train) / 6)\n    LR = 0.000125\n\n    if epoch != 0:\n        # Load Model Weights\n        model.load_weights('model.h5')    \n\n    model.compile(optimizer = Adam(learning_rate = LR), \n                  loss = 'binary_crossentropy',\n                  metrics = ['acc', tf.keras.metrics.AUC()])\n    \n    # Train Model\n    model.fit_generator(generator = data_generator_train,\n                        validation_data = data_generator_val,\n                        steps_per_epoch = TRAIN_STEPS,\n                        epochs = 1,\n                        callbacks = [ModelCheckpointFull('model.h5')],\n                        verbose = 1)\n    \n    # Starting with the 1th epoch we create predictions for the test set on each epoch\n    if epoch >= 2:\n        preds = predictions(test_df, model)\n        submission_predictions.append(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.iloc[:, :] = np.average(submission_predictions, axis = 0, weights = [2**i for i in range(len(submission_predictions))])\ntest_df = test_df.stack().reset_index()\ntest_df.insert(loc = 0, column = 'ID', value = test_df['Image'].astype(str) + \"_\" + test_df['Diagnosis'])\ntest_df = test_df.drop([\"Image\", \"Diagnosis\"], axis=1)\ntest_df.to_csv('submission.csv', index = False)\nprint(test_df.head(12))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.to_csv('new_submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}